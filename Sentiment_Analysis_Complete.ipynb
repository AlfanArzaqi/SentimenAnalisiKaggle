{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "SENTIMENT ANALYSIS - TWITTER DATASET\n",
    "============================================================================\n",
    "Target: Training & Testing Accuracy > 92%\n",
    "\n",
    "Models:\n",
    "1. Optimized Logistic Regression + TF-IDF\n",
    "2. BiLSTM + Attention Mechanism  \n",
    "3. Multi-Filter CNN\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Bidirectional, Dense, \n",
    "    Dropout, Conv1D, GlobalMaxPooling1D, Concatenate, Layer\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Note: Kaggle API Credentials\n",
    "\n",
    "The automatic dataset download feature requires **Kaggle API credentials**.\n",
    "\n",
    "### Setup Instructions:\n",
    "\n",
    "1. **Create a Kaggle Account**: If you don't have one, sign up at [kaggle.com](https://www.kaggle.com/)\n",
    "\n",
    "2. **Generate API Token**:\n",
    "   - Go to your Kaggle account settings\n",
    "   - Scroll to the \"API\" section\n",
    "   - Click \"Create New API Token\"\n",
    "   - This will download `kaggle.json`\n",
    "\n",
    "3. **Place the credentials file**:\n",
    "   - **Linux/Mac**: `~/.kaggle/kaggle.json`\n",
    "   - **Windows**: `C:\\Users\\<YourUsername>\\.kaggle\\kaggle.json`\n",
    "\n",
    "4. **Set permissions** (Linux/Mac only):\n",
    "   ```bash\n",
    "   chmod 600 ~/.kaggle/kaggle.json\n",
    "   ```\n",
    "\n",
    "### Alternative: Manual Download\n",
    "\n",
    "If you prefer manual download or encounter issues:\n",
    "\n",
    "1. Visit: [Twitter Entity Sentiment Analysis Dataset](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)\n",
    "2. Download the dataset\n",
    "3. Extract `twitter_training.csv` to this notebook's directory\n",
    "4. The notebook will automatically detect and use the local file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD DATASET FROM KAGGLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üì• Downloading dataset from Kaggle...\")\n",
    "\n",
    "try:\n",
    "    import kagglehub\n",
    "    \n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"jp797498e/twitter-entity-sentiment-analysis\")\n",
    "    \n",
    "    print(f\"‚úÖ Dataset downloaded successfully!\")\n",
    "    print(f\"üìÅ Path to dataset files: {path}\")\n",
    "    \n",
    "    # List files in the downloaded directory\n",
    "    files = os.listdir(path)\n",
    "    print(f\"\\nüìÑ Available files:\")\n",
    "    for file in files:\n",
    "        print(f\"   - {file}\")\n",
    "    \n",
    "    # Set the dataset path\n",
    "    DATASET_PATH = path\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  kagglehub not installed. Installing...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.check_call(['pip', 'install', 'kagglehub'])\n",
    "        print(\"‚úÖ kagglehub installed successfully!\")\n",
    "        \n",
    "        # Now try to download\n",
    "        import kagglehub\n",
    "        path = kagglehub.dataset_download(\"jp797498e/twitter-entity-sentiment-analysis\")\n",
    "        print(f\"‚úÖ Dataset downloaded successfully!\")\n",
    "        print(f\"üìÅ Path to dataset files: {path}\")\n",
    "        DATASET_PATH = path\n",
    "        \n",
    "    except Exception as install_error:\n",
    "        print(f\"‚ùå Error during installation or download: {install_error}\")\n",
    "        print(\"‚ö†Ô∏è  Falling back to local file...\")\n",
    "        DATASET_PATH = \".\"\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Falling back to local file...\")\n",
    "    DATASET_PATH = \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìÇ Loading dataset...\")\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file = os.path.join(DATASET_PATH, 'twitter_training.csv')\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(csv_file):\n",
    "    print(f\"‚ö†Ô∏è  File not found at {csv_file}\")\n",
    "    print(\"    Trying alternative filename...\")\n",
    "    \n",
    "    # List all CSV files in the directory\n",
    "    try:\n",
    "        csv_files = [f for f in os.listdir(DATASET_PATH) if f.endswith('.csv')]\n",
    "        if csv_files:\n",
    "            print(f\"    Found CSV files: {csv_files}\")\n",
    "            csv_file = os.path.join(DATASET_PATH, csv_files[0])\n",
    "            print(f\"    Using: {csv_file}\")\n",
    "        else:\n",
    "            # Fallback to local file\n",
    "            csv_file = 'twitter_training.csv'\n",
    "            print(f\"    Using local file: {csv_file}\")\n",
    "    except (OSError, FileNotFoundError) as e:\n",
    "        print(f\"    Error listing directory: {e}\")\n",
    "        # Fallback to local file\n",
    "        csv_file = 'twitter_training.csv'\n",
    "        print(f\"    Using local file: {csv_file}\")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(csv_file, header=None, \n",
    "                   names=['Tweet ID', 'entity', 'sentiment', 'Tweet content'])\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"   File: {csv_file}\")\n",
    "print(f\"\\nDataset shape: {data.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(data.head())\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DATA EXPLORATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Original sentiment distribution\n",
    "print(\"\\n=== ORIGINAL SENTIMENT DISTRIBUTION ===\")\n",
    "print(data['sentiment'].value_counts())\n",
    "\n",
    "# Visualize original distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original distribution\n",
    "data['sentiment'].value_counts().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Original Sentiment Distribution', fontsize=14, weight='bold')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['Tweet ID', 'entity'])\n",
    "\n",
    "# Remove missing values\n",
    "data = data.dropna()\n",
    "print(f\"\\n‚úÖ Data after removing NaN: {data.shape}\")\n",
    "\n",
    "# Merge Irrelevant to Neutral (4 ‚Üí 3 classes)\n",
    "data['sentiment'] = data['sentiment'].replace('Irrelevant', 'Neutral')\n",
    "\n",
    "print(\"\\n=== MERGED SENTIMENT DISTRIBUTION (3 CLASSES) ===\")\n",
    "print(data['sentiment'].value_counts())\n",
    "\n",
    "# Visualize merged distribution\n",
    "data['sentiment'].value_counts().plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Sentiment Distribution (3 Classes)', fontsize=14, weight='bold')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß DATA PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define preprocessing functions\n",
    "def lowercase(text):\n",
    "    \"\"\"Convert to lowercase\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    \"\"\"Remove URLs, mentions, retweets, special characters\"\"\"\n",
    "    text = re.sub(r'pic\\.twitter\\.com\\.[^\\s]+', '', text)\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\brt\\b', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'@[^\\s]+', ' ', text)\n",
    "    text = re.sub(r'(.)\\1\\1+', r'\\1\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\xe2]+', ' ', text)\n",
    "    text = re.sub(r':', '', text)\n",
    "    text = re.sub(r'‚Äö√Ñ¬∂', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_nonalphanumeric(text):\n",
    "    \"\"\"Remove non-alphanumeric characters\"\"\"\n",
    "    text = re.sub(r'[^0-9a-zA-Z]+', ' ', text)\n",
    "    text = re.sub(r'00', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove English stopwords\"\"\"\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    return [t for t in tokens if t not in english_stopwords]\n",
    "\n",
    "def stemming(text):\n",
    "    \"\"\"Apply stemming\"\"\"\n",
    "    snowball = SnowballStemmer(language='english')\n",
    "    return snowball.stem(text)\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return None\n",
    "    \n",
    "    text = lowercase(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = remove_nonalphanumeric(text)\n",
    "    text = stemming(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    return ' '.join(tokens) if tokens else None\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"\\n‚è≥ Preprocessing texts...\")\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "data['cleaned_text'] = data['Tweet content'].progress_apply(preprocess)\n",
    "\n",
    "# Remove empty texts\n",
    "data = data.dropna(subset=['cleaned_text'])\n",
    "data = data[data['cleaned_text'] != '']\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete! Final shape: {data.shape}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n=== PREPROCESSING EXAMPLES ===\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. Original: {data.iloc[i]['Tweet content'][:100]}...\")\n",
    "    print(f\"   Cleaned:  {data.iloc[i]['cleaned_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARE DATA FOR MODELING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ PREPARE DATA FOR MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract features and labels\n",
    "X = data['cleaned_text'].values\n",
    "y = data['sentiment'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n=== LABEL ENCODING ===\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "print(f\"Encoded: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "print(f\"\\nTotal samples: {len(X)}\")\n",
    "print(f\"Class distribution:\")\n",
    "for cls in label_encoder.classes_:\n",
    "    count = (y == cls).sum()\n",
    "    print(f\"  {cls}: {count} ({count/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: LOGISTIC REGRESSION + TF-IDF\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ MODEL 1: LOGISTIC REGRESSION + TF-IDF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data (80/20)\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "print(f\"\\nData split: {len(X_train_lr)} train, {len(X_test_lr)} test\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"\\n‚è≥ Creating TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\w{2,}',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lr)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_lr)\n",
    "print(f\"‚úÖ TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Train Logistic Regression\n",
    "print(\"\\n‚è≥ Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    solver='saga',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_tfidf, y_train_lr)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_tfidf)\n",
    "y_test_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "train_acc_lr = accuracy_score(y_train_lr, y_train_pred_lr)\n",
    "test_acc_lr = accuracy_score(y_test_lr, y_test_pred_lr)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üìä RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training Accuracy:   {train_acc_lr*100:.2f}%\")\n",
    "print(f\"Testing Accuracy:    {test_acc_lr*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test_lr, y_test_pred_lr, \n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test_lr, y_test_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, weight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Model 1 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 2: BiLSTM + ATTENTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ MODEL 2: BiLSTM + ATTENTION MECHANISM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data (80/20)\n",
    "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "print(\"\\n‚è≥ Tokenizing texts...\")\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_dl)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_dl)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_dl)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "print(f\"‚úÖ Sequences shape: {X_train_pad.shape}\")\n",
    "\n",
    "# One-hot encode\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_dl, num_classes)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test_dl, num_classes)\n",
    "\n",
    "# Define Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"Bahdanau Attention\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name='attention_weight',\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "# Build model\n",
    "print(\"\\n‚è≥ Building BiLSTM + Attention model...\")\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 128, input_length=max_len)(input_layer)\n",
    "bilstm = Bidirectional(LSTM(64, return_sequences=True, dropout=0.3))(embedding)\n",
    "attention = AttentionLayer()(bilstm)\n",
    "dense1 = Dense(128, activation='relu')(attention)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(64, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.3)(dense2)\n",
    "output = Dense(num_classes, activation='softmax')(dropout2)\n",
    "\n",
    "bilstm_model = Model(inputs=input_layer, outputs=output)\n",
    "bilstm_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(bilstm_model.summary())\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "\n",
    "# Train\n",
    "print(\"\\n‚è≥ Training BiLSTM + Attention...\")\n",
    "history_bilstm = bilstm_model.fit(\n",
    "    X_train_pad, y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_loss_bilstm, train_acc_bilstm = bilstm_model.evaluate(X_train_pad, y_train_cat, verbose=0)\n",
    "test_loss_bilstm, test_acc_bilstm = bilstm_model.evaluate(X_test_pad, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üìä RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training Accuracy:   {train_acc_bilstm*100:.2f}%\")\n",
    "print(f\"Testing Accuracy:    {test_acc_bilstm*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Plot history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history_bilstm.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0].plot(history_bilstm.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].set_title('BiLSTM + Attention: Accuracy', fontsize=14, weight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_bilstm.history['loss'], label='Train', linewidth=2)\n",
    "axes[1].plot(history_bilstm.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_title('BiLSTM + Attention: Loss', fontsize=14, weight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_pred_bilstm = bilstm_model.predict(X_test_pad, verbose=0)\n",
    "y_test_pred_bilstm = np.argmax(y_test_pred_bilstm, axis=1)\n",
    "\n",
    "cm_bilstm = confusion_matrix(y_test_dl, y_test_pred_bilstm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_bilstm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - BiLSTM + Attention', fontsize=14, weight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Model 2 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 3: MULTI-FILTER CNN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ MODEL 3: MULTI-FILTER CNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data (70/30)\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42\n",
    ")\n",
    "print(f\"\\nData split: {len(X_train_cnn)} train, {len(X_test_cnn)} test\")\n",
    "\n",
    "# Tokenization (reuse tokenizer)\n",
    "X_train_cnn_seq = tokenizer.texts_to_sequences(X_train_cnn)\n",
    "X_test_cnn_seq = tokenizer.texts_to_sequences(X_test_cnn)\n",
    "\n",
    "X_train_cnn_pad = pad_sequences(X_train_cnn_seq, maxlen=max_len, padding='post')\n",
    "X_test_cnn_pad = pad_sequences(X_test_cnn_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "y_train_cnn_cat = tf.keras.utils.to_categorical(y_train_cnn, num_classes)\n",
    "y_test_cnn_cat = tf.keras.utils.to_categorical(y_test_cnn, num_classes)\n",
    "\n",
    "# Build model\n",
    "print(\"\\n‚è≥ Building Multi-Filter CNN...\")\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 128, input_length=max_len)(input_layer)\n",
    "\n",
    "# Multiple filter sizes\n",
    "filter_sizes = [2, 3, 4, 5]\n",
    "conv_layers = []\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    conv = Conv1D(128, kernel_size=filter_size, activation='relu')(embedding)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    conv_layers.append(pool)\n",
    "\n",
    "concat = Concatenate()(conv_layers)\n",
    "dense1 = Dense(256, activation='relu')(concat)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(128, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.3)(dense2)\n",
    "output = Dense(num_classes, activation='softmax')(dropout2)\n",
    "\n",
    "cnn_model = Model(inputs=input_layer, outputs=output)\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(cnn_model.summary())\n",
    "\n",
    "# Train\n",
    "print(\"\\n‚è≥ Training Multi-Filter CNN...\")\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train_cnn_pad, y_train_cnn_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_loss_cnn, train_acc_cnn = cnn_model.evaluate(X_train_cnn_pad, y_train_cnn_cat, verbose=0)\n",
    "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(X_test_cnn_pad, y_test_cnn_cat, verbose=0)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üìä RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training Accuracy:   {train_acc_cnn*100:.2f}%\")\n",
    "print(f\"Testing Accuracy:    {test_acc_cnn*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Plot history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history_cnn.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0].plot(history_cnn.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].set_title('Multi-Filter CNN: Accuracy', fontsize=14, weight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_cnn.history['loss'], label='Train', linewidth=2)\n",
    "axes[1].plot(history_cnn.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_title('Multi-Filter CNN: Loss', fontsize=14, weight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_pred_cnn = cnn_model.predict(X_test_cnn_pad, verbose=0)\n",
    "y_test_pred_cnn = np.argmax(y_test_pred_cnn, axis=1)\n",
    "\n",
    "cm_cnn = confusion_matrix(y_test_cnn, y_test_pred_cnn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Multi-Filter CNN', fontsize=14, weight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Model 3 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression + TF-IDF',\n",
    "        'BiLSTM + Attention',\n",
    "        'Multi-Filter CNN'\n",
    "    ],\n",
    "    'Train Accuracy (%)': [\n",
    "        train_acc_lr * 100,\n",
    "        train_acc_bilstm * 100,\n",
    "        train_acc_cnn * 100\n",
    "    ],\n",
    "    'Test Accuracy (%)': [\n",
    "        test_acc_lr * 100,\n",
    "        test_acc_bilstm * 100,\n",
    "        test_acc_cnn * 100\n",
    "    ],\n",
    "    'Data Split': ['80/20', '80/20', '70/30'],\n",
    "    'Feature Extraction': ['TF-IDF (1-2gram)', 'Trainable Embedding', 'Trainable Embedding']\n",
    "})\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON TABLE ===\")\n",
    "display(comparison)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison['Train Accuracy (%)'], \n",
    "               width, label='Train Accuracy', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, comparison['Test Accuracy (%)'], \n",
    "               width, label='Test Accuracy', color='coral')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, weight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, weight='bold')\n",
    "ax.set_title('Model Comparison: Train vs Test Accuracy', fontsize=14, weight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check target achievement\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ TARGET ACHIEVEMENT CHECK (>92%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_threshold = 92.0\n",
    "models_above_92 = []\n",
    "\n",
    "for idx, row in comparison.iterrows():\n",
    "    model_name = row['Model']\n",
    "    train_acc = row['Train Accuracy (%)']\n",
    "    test_acc = row['Test Accuracy (%)']\n",
    "    \n",
    "    if train_acc >= target_threshold and test_acc >= target_threshold:\n",
    "        models_above_92.append(model_name)\n",
    "        status = \"‚úÖ ACHIEVED\"\n",
    "    else:\n",
    "        status = \"‚ùå NOT ACHIEVED\"\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Train: {train_acc:.2f}% | Test: {test_acc:.2f}% | {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if models_above_92:\n",
    "    print(f\"‚úÖ {len(models_above_92)} model(s) achieved >92% target!\")\n",
    "    for model in models_above_92:\n",
    "        print(f\"   - {model}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  None of the models achieved >92% target yet.\")\n",
    "    print(\"   Consider: more epochs, hyperparameter tuning, or data augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE / TESTING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÆ INFERENCE / TESTING EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    \"I absolutely love this product! Best purchase ever!\",\n",
    "    \"This is terrible. Worst experience of my life.\",\n",
    "    \"It's okay, nothing special really.\",\n",
    "    \"Amazing quality and fast delivery! Highly recommend!\",\n",
    "    \"Very disappointed. Would not buy again.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== TESTING NEW SAMPLES ===\\n\")\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Sample {i}: {sample}\")\n",
    "    \n",
    "    # Preprocess\n",
    "    cleaned = preprocess(sample)\n",
    "    if cleaned is None or cleaned == '':\n",
    "        print(\"  ‚ö†Ô∏è  Preprocessing failed\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Cleaned: {cleaned}\")\n",
    "    print(f\"\\nüìä Predictions:\")\n",
    "    \n",
    "    # Model 1: Logistic Regression\n",
    "    sample_tfidf = tfidf_vectorizer.transform([cleaned])\n",
    "    pred_lr = lr_model.predict(sample_tfidf)[0]\n",
    "    pred_lr_proba = lr_model.predict_proba(sample_tfidf)[0]\n",
    "    \n",
    "    print(f\"  LR:     {label_encoder.classes_[pred_lr]:12} (confidence: {pred_lr_proba[pred_lr]:.3f})\")\n",
    "    \n",
    "    # Model 2: BiLSTM\n",
    "    sample_seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    sample_pad = pad_sequences(sample_seq, maxlen=max_len, padding='post')\n",
    "    pred_bilstm = bilstm_model.predict(sample_pad, verbose=0)\n",
    "    pred_bilstm_class = np.argmax(pred_bilstm[0])\n",
    "    \n",
    "    print(f\"  BiLSTM: {label_encoder.classes_[pred_bilstm_class]:12} (confidence: {pred_bilstm[0][pred_bilstm_class]:.3f})\")\n",
    "    \n",
    "    # Model 3: CNN\n",
    "    pred_cnn = cnn_model.predict(sample_pad, verbose=0)\n",
    "    pred_cnn_class = np.argmax(pred_cnn[0])\n",
    "    \n",
    "    print(f\"  CNN:    {label_encoder.classes_[pred_cnn_class]:12} (confidence: {pred_cnn[0][pred_cnn_class]:.3f})\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Save Logistic Regression\n",
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "print(\"‚úÖ Saved: logistic_regression_model.pkl\")\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(\"‚úÖ Saved: tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Save Deep Learning models\n",
    "bilstm_model.save('bilstm_attention_model.h5')\n",
    "print(\"‚úÖ Saved: bilstm_attention_model.h5\")\n",
    "\n",
    "cnn_model.save('multi_filter_cnn_model.h5')\n",
    "print(\"‚úÖ Saved: multi_filter_cnn_model.h5\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w') as f:\n",
    "    json.dump(tokenizer_json, f)\n",
    "print(\"‚úÖ Saved: tokenizer.json\")\n",
    "\n",
    "# Save label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"‚úÖ Saved: label_encoder.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALL MODELS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Final Summary:\")\n",
    "print(f\"   Dataset: {len(data)} samples\")\n",
    "print(f\"   Classes: {len(label_encoder.classes_)} ({', '.join(label_encoder.classes_)})\")\n",
    "print(f\"   Models trained: 3\")\n",
    "print(f\"   Models achieving >92%: {len(models_above_92)}\")\n",
    "\n",
    "print(\"\\nüéâ SENTIMENT ANALYSIS PROJECT COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}