{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcda Setup Instructions",
    "",
    "## Running This Notebook",
    "",
    "### Option 1: Google Colab (Recommended for Grading)",
    "1. Upload this notebook to Google Colab",
    "2. Run all cells - the dataset will be automatically cloned from the repository",
    "3. No additional setup required!",
    "",
    "### Option 2: Local Environment",
    "1. Clone the repository:",
    "   ```bash",
    "   git clone https://github.com/AlfanArzaqi/SentimenAnalisiKaggle.git",
    "   cd SentimenAnalisiKaggle",
    "   ```",
    "",
    "2. Install dependencies:",
    "   ```bash",
    "   pip install -r requirements.txt",
    "   ```",
    "",
    "3. Run the notebook - dataset will be loaded from the `dataset/` folder",
    "",
    "## Dataset Information",
    "- **Source**: Twitter Entity Sentiment Analysis (Kaggle)",
    "- **Location**: `dataset/` folder in this repository",
    "- **Files**:",
    "  - `twitter_training.csv` - Main training dataset",
    "  - `twitter_validation.csv` - Test/validation dataset (if applicable)",
    "",
    "## Important Notes",
    "- \u2705 No Kaggle API credentials required",
    "- \u2705 Dataset included in repository",
    "- \u2705 Works on Google Colab and local environments",
    "- \u2705 Automatic environment detection",
    "",
    "---",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "SENTIMENT ANALYSIS - TWITTER DATASET\n",
    "============================================================================\n",
    "Target: Training & Testing Accuracy > 92%\n",
    "\n",
    "Models:\n",
    "1. Optimized Logistic Regression + TF-IDF\n",
    "2. BiLSTM + Attention Mechanism  \n",
    "3. Multi-Filter CNN\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Bidirectional, Dense, \n",
    "    Dropout, Conv1D, GlobalMaxPooling1D, Concatenate, Layer\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\u2705 All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================",
    "# SETUP ENVIRONMENT - AUTO-DETECT COLAB vs LOCAL",
    "# ============================================================================",
    "",
    "import os",
    "import sys",
    "",
    "# Detect if running in Google Colab",
    "try:",
    "    import google.colab",
    "    IN_COLAB = True",
    "    print(\"\ud83d\udd0d Environment: Google Colab\")",
    "except ImportError:",
    "    IN_COLAB = False",
    "    print(\"\ud83d\udd0d Environment: Local\")",
    "",
    "# Setup dataset path",
    "if IN_COLAB:",
    "    print(\"\\n\ud83d\udce5 Setting up Google Colab environment...\")",
    "    ",
    "    # Check if repository is already cloned",
    "    if not os.path.exists('/content/SentimenAnalisiKaggle'):",
    "        print(\"\u23f3 Cloning repository from GitHub...\")",
    "        !git clone https://github.com/AlfanArzaqi/SentimenAnalisiKaggle.git",
    "        print(\"\u2705 Repository cloned successfully!\")",
    "    else:",
    "        print(\"\u2705 Repository already exists\")",
    "    ",
    "    # Set base path to cloned repository",
    "    BASE_PATH = '/content/SentimenAnalisiKaggle'",
    "    os.chdir(BASE_PATH)",
    "    print(f\"\ud83d\udcc1 Working directory: {os.getcwd()}\")",
    "    ",
    "else:",
    "    # Local environment - assume dataset folder is in same directory",
    "    BASE_PATH = os.path.dirname(os.path.abspath('__file__')) if '__file__' in globals() else os.getcwd()",
    "    print(f\"\ud83d\udcc1 Working directory: {BASE_PATH}\")",
    "",
    "# Set dataset path",
    "DATASET_PATH = os.path.join(BASE_PATH, 'dataset')",
    "",
    "# Verify dataset folder exists",
    "if os.path.exists(DATASET_PATH):",
    "    print(f\"\u2705 Dataset folder found: {DATASET_PATH}\")",
    "    ",
    "    # List available files",
    "    dataset_files = os.listdir(DATASET_PATH)",
    "    print(f\"\\n\ud83d\udcc4 Available dataset files:\")",
    "    for file in dataset_files:",
    "        file_path = os.path.join(DATASET_PATH, file)",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB",
    "        print(f\"   - {file} ({file_size:.2f} MB)\")",
    "else:",
    "    print(f\"\u26a0\ufe0f  Dataset folder not found at: {DATASET_PATH}\")",
    "    print(\"   Please ensure the 'dataset' folder exists with CSV files.\")",
    "    # Set to current directory as fallback",
    "    DATASET_PATH = \".\"",
    "",
    "print(\"\\n\" + \"=\"*70)",
    "print(\"\u2705 Setup complete! Ready to load data.\")",
    "print(\"=\"*70)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================",
    "# LOAD DATA",
    "# ============================================================================",
    "",
    "print(\"\ud83d\udcc2 Loading dataset from repository...\")",
    "",
    "# Construct the full path to the CSV file",
    "train_csv = os.path.join(DATASET_PATH, 'twitter_training.csv')",
    "",
    "# Check if file exists",
    "if not os.path.exists(train_csv):",
    "    print(f\"\u26a0\ufe0f  File not found at: {train_csv}\")",
    "    print(\"    Searching for CSV files...\")",
    "    ",
    "    # Try to find any CSV file with 'train' in the name",
    "    try:",
    "        csv_files = [f for f in os.listdir(DATASET_PATH) if f.endswith('.csv')]",
    "        train_files = [f for f in csv_files if 'train' in f.lower()]",
    "        ",
    "        if train_files:",
    "            train_csv = os.path.join(DATASET_PATH, train_files[0])",
    "            print(f\"    \u2705 Found: {train_files[0]}\")",
    "        elif csv_files:",
    "            train_csv = os.path.join(DATASET_PATH, csv_files[0])",
    "            print(f\"    \u2705 Using: {csv_files[0]}\")",
    "        else:",
    "            raise FileNotFoundError(\"No CSV files found in dataset folder\")",
    "    except Exception as e:",
    "        print(f\"    \u274c Error: {e}\")",
    "        print(\"\\n\u26a0\ufe0f  Please ensure dataset files are in the 'dataset' folder:\")",
    "        print(\"    - twitter_training.csv (required)\")",
    "        print(\"    - twitter_validation.csv (optional)\")",
    "        raise",
    "",
    "# Load dataset",
    "print(f\"\\n\u23f3 Loading: {os.path.basename(train_csv)}\")",
    "data = pd.read_csv(train_csv, header=None, ",
    "                   names=['Tweet ID', 'entity', 'sentiment', 'Tweet content'])",
    "",
    "print(f\"\u2705 Dataset loaded successfully!\")",
    "print(f\"   Source: {train_csv}\")",
    "print(f\"\\nDataset shape: {data.shape}\")",
    "print(f\"\\nFirst 5 rows:\")",
    "display(data.head())",
    "",
    "print(f\"\\nDataset info:\")",
    "data.info()",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca DATA EXPLORATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Original sentiment distribution\n",
    "print(\"\\n=== ORIGINAL SENTIMENT DISTRIBUTION ===\")\n",
    "print(data['sentiment'].value_counts())\n",
    "\n",
    "# Visualize original distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original distribution\n",
    "data['sentiment'].value_counts().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Original Sentiment Distribution', fontsize=14, weight='bold')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['Tweet ID', 'entity'])\n",
    "\n",
    "# Remove missing values\n",
    "data = data.dropna()\n",
    "print(f\"\\n\u2705 Data after removing NaN: {data.shape}\")\n",
    "\n",
    "# Merge Irrelevant to Neutral (4 \u2192 3 classes)\n",
    "data['sentiment'] = data['sentiment'].replace('Irrelevant', 'Neutral')\n",
    "\n",
    "print(\"\\n=== MERGED SENTIMENT DISTRIBUTION (3 CLASSES) ===\")\n",
    "print(data['sentiment'].value_counts())\n",
    "\n",
    "# Visualize merged distribution\n",
    "data['sentiment'].value_counts().plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Sentiment Distribution (3 Classes)', fontsize=14, weight='bold')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udd27 DATA PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define preprocessing functions\n",
    "def lowercase(text):\n",
    "    \"\"\"Convert to lowercase\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    \"\"\"Remove URLs, mentions, retweets, special characters\"\"\"\n",
    "    text = re.sub(r'pic\\.twitter\\.com\\.[^\\s]+', '', text)\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\brt\\b', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'@[^\\s]+', ' ', text)\n",
    "    text = re.sub(r'(.)\\1\\1+', r'\\1\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\xe2]+', ' ', text)\n",
    "    text = re.sub(r':', '', text)\n",
    "    text = re.sub(r'\u201a\u00c4\u00b6', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_nonalphanumeric(text):\n",
    "    \"\"\"Remove non-alphanumeric characters\"\"\"\n",
    "    text = re.sub(r'[^0-9a-zA-Z]+', ' ', text)\n",
    "    text = re.sub(r'00', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove English stopwords\"\"\"\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    return [t for t in tokens if t not in english_stopwords]\n",
    "\n",
    "def stemming(text):\n",
    "    \"\"\"Apply stemming\"\"\"\n",
    "    snowball = SnowballStemmer(language='english')\n",
    "    return snowball.stem(text)\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return None\n",
    "    \n",
    "    text = lowercase(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = remove_nonalphanumeric(text)\n",
    "    text = stemming(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    return ' '.join(tokens) if tokens else None\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"\\n\u23f3 Preprocessing texts...\")\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "data['cleaned_text'] = data['Tweet content'].progress_apply(preprocess)\n",
    "\n",
    "# Remove empty texts\n",
    "data = data.dropna(subset=['cleaned_text'])\n",
    "data = data[data['cleaned_text'] != '']\n",
    "\n",
    "print(f\"\\n\u2705 Preprocessing complete! Final shape: {data.shape}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n=== PREPROCESSING EXAMPLES ===\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. Original: {data.iloc[i]['Tweet content'][:100]}...\")\n",
    "    print(f\"   Cleaned:  {data.iloc[i]['cleaned_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARE DATA FOR MODELING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udfaf PREPARE DATA FOR MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract features and labels\n",
    "X = data['cleaned_text'].values\n",
    "y = data['sentiment'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n=== LABEL ENCODING ===\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "print(f\"Encoded: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "print(f\"\\nTotal samples: {len(X)}\")\n",
    "print(f\"Class distribution:\")\n",
    "for cls in label_encoder.classes_:\n",
    "    count = (y == cls).sum()\n",
    "    print(f\"  {cls}: {count} ({count/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: LOGISTIC REGRESSION + TF-IDF\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83e\udd16 MODEL 1: LOGISTIC REGRESSION + TF-IDF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data (80/20)\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "print(f\"\\nData split: {len(X_train_lr)} train, {len(X_test_lr)} test\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"\\n\u23f3 Creating TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\w{2,}',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lr)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_lr)\n",
    "print(f\"\u2705 TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Train Logistic Regression\n",
    "print(\"\\n\u23f3 Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    solver='saga',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_tfidf, y_train_lr)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_tfidf)\n",
    "y_test_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "train_acc_lr = accuracy_score(y_train_lr, y_train_pred_lr)\n",
    "test_acc_lr = accuracy_score(y_test_lr, y_test_pred_lr)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"\ud83d\udcca RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training Accuracy:   {train_acc_lr*100:.2f}%\")\n",
    "print(f\"Testing Accuracy:    {test_acc_lr*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test_lr, y_test_pred_lr, \n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test_lr, y_test_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, weight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2705 Model 1 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 2: BiLSTM + ATTENTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83e\udd16 MODEL 2: BiLSTM + ATTENTION MECHANISM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data (80/20)\n",
    "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "print(\"\\n\u23f3 Tokenizing texts...\")\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_dl)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_dl)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_dl)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "print(f\"\u2705 Sequences shape: {X_train_pad.shape}\")\n",
    "\n",
    "# One-hot encode\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_dl, num_classes)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test_dl, num_classes)\n",
    "\n",
    "# Define Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"Bahdanau Attention\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name='attention_weight',\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "# Build model\n",
    "print(\"\\n\u23f3 Building BiLSTM + Attention model...\")\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 128, input_length=max_len)(input_layer)\n",
    "bilstm = Bidirectional(LSTM(64, return_sequences=True, dropout=0.3))(embedding)\n",
    "attention = AttentionLayer()(bilstm)\n",
    "dense1 = Dense(128, activation='relu')(attention)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(64, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.3)(dense2)\n",
    "output = Dense(num_classes, activation='softmax')(dropout2)\n",
    "\n",
    "bilstm_model = Model(inputs=input_layer, outputs=output)\n",
    "bilstm_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(bilstm_model.summary())\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "\n",
    "# Train\n",
    "print(\"\\n\u23f3 Training BiLSTM + Attention...\")\n",
    "history_bilstm = bilstm_model.fit(\n",
    "    X_train_pad, y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_loss_bilstm, train_acc_bilstm = bilstm_model.evaluate(X_train_pad, y_train_cat, verbose=0)\n",
    "test_loss_bilstm, test_acc_bilstm = bilstm_model.evaluate(X_test_pad, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"\ud83d\udcca RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training Accuracy:   {train_acc_bilstm*100:.2f}%\")\n",
    "print(f\"Testing Accuracy:    {test_acc_bilstm*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Plot history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history_bilstm.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0].plot(history_bilstm.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].set_title('BiLSTM + Attention: Accuracy', fontsize=14, weight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_bilstm.history['loss'], label='Train', linewidth=2)\n",
    "axes[1].plot(history_bilstm.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_title('BiLSTM + Attention: Loss', fontsize=14, weight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_pred_bilstm = bilstm_model.predict(X_test_pad, verbose=0)\n",
    "y_test_pred_bilstm = np.argmax(y_test_pred_bilstm, axis=1)\n",
    "\n",
    "cm_bilstm = confusion_matrix(y_test_dl, y_test_pred_bilstm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_bilstm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - BiLSTM + Attention', fontsize=14, weight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2705 Model 2 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 3: MULTI-FILTER CNN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83e\udd16 MODEL 3: MULTI-FILTER CNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data (70/30)\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42\n",
    ")\n",
    "print(f\"\\nData split: {len(X_train_cnn)} train, {len(X_test_cnn)} test\")\n",
    "\n",
    "# Tokenization (reuse tokenizer)\n",
    "X_train_cnn_seq = tokenizer.texts_to_sequences(X_train_cnn)\n",
    "X_test_cnn_seq = tokenizer.texts_to_sequences(X_test_cnn)\n",
    "\n",
    "X_train_cnn_pad = pad_sequences(X_train_cnn_seq, maxlen=max_len, padding='post')\n",
    "X_test_cnn_pad = pad_sequences(X_test_cnn_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "y_train_cnn_cat = tf.keras.utils.to_categorical(y_train_cnn, num_classes)\n",
    "y_test_cnn_cat = tf.keras.utils.to_categorical(y_test_cnn, num_classes)\n",
    "\n",
    "# Build model\n",
    "print(\"\\n\u23f3 Building Multi-Filter CNN...\")\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 128, input_length=max_len)(input_layer)\n",
    "\n",
    "# Multiple filter sizes\n",
    "filter_sizes = [2, 3, 4, 5]\n",
    "conv_layers = []\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    conv = Conv1D(128, kernel_size=filter_size, activation='relu')(embedding)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    conv_layers.append(pool)\n",
    "\n",
    "concat = Concatenate()(conv_layers)\n",
    "dense1 = Dense(256, activation='relu')(concat)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(128, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.3)(dense2)\n",
    "output = Dense(num_classes, activation='softmax')(dropout2)\n",
    "\n",
    "cnn_model = Model(inputs=input_layer, outputs=output)\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(cnn_model.summary())\n",
    "\n",
    "# Train\n",
    "print(\"\\n\u23f3 Training Multi-Filter CNN...\")\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train_cnn_pad, y_train_cnn_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_loss_cnn, train_acc_cnn = cnn_model.evaluate(X_train_cnn_pad, y_train_cnn_cat, verbose=0)\n",
    "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(X_test_cnn_pad, y_test_cnn_cat, verbose=0)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"\ud83d\udcca RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training Accuracy:   {train_acc_cnn*100:.2f}%\")\n",
    "print(f\"Testing Accuracy:    {test_acc_cnn*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Plot history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history_cnn.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0].plot(history_cnn.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].set_title('Multi-Filter CNN: Accuracy', fontsize=14, weight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_cnn.history['loss'], label='Train', linewidth=2)\n",
    "axes[1].plot(history_cnn.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_title('Multi-Filter CNN: Loss', fontsize=14, weight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_pred_cnn = cnn_model.predict(X_test_cnn_pad, verbose=0)\n",
    "y_test_pred_cnn = np.argmax(y_test_pred_cnn, axis=1)\n",
    "\n",
    "cm_cnn = confusion_matrix(y_test_cnn, y_test_pred_cnn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Multi-Filter CNN', fontsize=14, weight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2705 Model 3 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression + TF-IDF',\n",
    "        'BiLSTM + Attention',\n",
    "        'Multi-Filter CNN'\n",
    "    ],\n",
    "    'Train Accuracy (%)': [\n",
    "        train_acc_lr * 100,\n",
    "        train_acc_bilstm * 100,\n",
    "        train_acc_cnn * 100\n",
    "    ],\n",
    "    'Test Accuracy (%)': [\n",
    "        test_acc_lr * 100,\n",
    "        test_acc_bilstm * 100,\n",
    "        test_acc_cnn * 100\n",
    "    ],\n",
    "    'Data Split': ['80/20', '80/20', '70/30'],\n",
    "    'Feature Extraction': ['TF-IDF (1-2gram)', 'Trainable Embedding', 'Trainable Embedding']\n",
    "})\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON TABLE ===\")\n",
    "display(comparison)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison['Train Accuracy (%)'], \n",
    "               width, label='Train Accuracy', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, comparison['Test Accuracy (%)'], \n",
    "               width, label='Test Accuracy', color='coral')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, weight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, weight='bold')\n",
    "ax.set_title('Model Comparison: Train vs Test Accuracy', fontsize=14, weight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check target achievement\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udfaf TARGET ACHIEVEMENT CHECK (>92%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_threshold = 92.0\n",
    "models_above_92 = []\n",
    "\n",
    "for idx, row in comparison.iterrows():\n",
    "    model_name = row['Model']\n",
    "    train_acc = row['Train Accuracy (%)']\n",
    "    test_acc = row['Test Accuracy (%)']\n",
    "    \n",
    "    if train_acc >= target_threshold and test_acc >= target_threshold:\n",
    "        models_above_92.append(model_name)\n",
    "        status = \"\u2705 ACHIEVED\"\n",
    "    else:\n",
    "        status = \"\u274c NOT ACHIEVED\"\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Train: {train_acc:.2f}% | Test: {test_acc:.2f}% | {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if models_above_92:\n",
    "    print(f\"\u2705 {len(models_above_92)} model(s) achieved >92% target!\")\n",
    "    for model in models_above_92:\n",
    "        print(f\"   - {model}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  None of the models achieved >92% target yet.\")\n",
    "    print(\"   Consider: more epochs, hyperparameter tuning, or data augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE / TESTING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udd2e INFERENCE / TESTING EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    \"I absolutely love this product! Best purchase ever!\",\n",
    "    \"This is terrible. Worst experience of my life.\",\n",
    "    \"It's okay, nothing special really.\",\n",
    "    \"Amazing quality and fast delivery! Highly recommend!\",\n",
    "    \"Very disappointed. Would not buy again.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== TESTING NEW SAMPLES ===\\n\")\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Sample {i}: {sample}\")\n",
    "    \n",
    "    # Preprocess\n",
    "    cleaned = preprocess(sample)\n",
    "    if cleaned is None or cleaned == '':\n",
    "        print(\"  \u26a0\ufe0f  Preprocessing failed\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Cleaned: {cleaned}\")\n",
    "    print(f\"\\n\ud83d\udcca Predictions:\")\n",
    "    \n",
    "    # Model 1: Logistic Regression\n",
    "    sample_tfidf = tfidf_vectorizer.transform([cleaned])\n",
    "    pred_lr = lr_model.predict(sample_tfidf)[0]\n",
    "    pred_lr_proba = lr_model.predict_proba(sample_tfidf)[0]\n",
    "    \n",
    "    print(f\"  LR:     {label_encoder.classes_[pred_lr]:12} (confidence: {pred_lr_proba[pred_lr]:.3f})\")\n",
    "    \n",
    "    # Model 2: BiLSTM\n",
    "    sample_seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    sample_pad = pad_sequences(sample_seq, maxlen=max_len, padding='post')\n",
    "    pred_bilstm = bilstm_model.predict(sample_pad, verbose=0)\n",
    "    pred_bilstm_class = np.argmax(pred_bilstm[0])\n",
    "    \n",
    "    print(f\"  BiLSTM: {label_encoder.classes_[pred_bilstm_class]:12} (confidence: {pred_bilstm[0][pred_bilstm_class]:.3f})\")\n",
    "    \n",
    "    # Model 3: CNN\n",
    "    pred_cnn = cnn_model.predict(sample_pad, verbose=0)\n",
    "    pred_cnn_class = np.argmax(pred_cnn[0])\n",
    "    \n",
    "    print(f\"  CNN:    {label_encoder.classes_[pred_cnn_class]:12} (confidence: {pred_cnn[0][pred_cnn_class]:.3f})\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\u2705 Inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcbe SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Save Logistic Regression\n",
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "print(\"\u2705 Saved: logistic_regression_model.pkl\")\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(\"\u2705 Saved: tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Save Deep Learning models\n",
    "bilstm_model.save('bilstm_attention_model.h5')\n",
    "print(\"\u2705 Saved: bilstm_attention_model.h5\")\n",
    "\n",
    "cnn_model.save('multi_filter_cnn_model.h5')\n",
    "print(\"\u2705 Saved: multi_filter_cnn_model.h5\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w') as f:\n",
    "    json.dump(tokenizer_json, f)\n",
    "print(\"\u2705 Saved: tokenizer.json\")\n",
    "\n",
    "# Save label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"\u2705 Saved: label_encoder.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 ALL MODELS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Final Summary:\")\n",
    "print(f\"   Dataset: {len(data)} samples\")\n",
    "print(f\"   Classes: {len(label_encoder.classes_)} ({', '.join(label_encoder.classes_)})\")\n",
    "print(f\"   Models trained: 3\")\n",
    "print(f\"   Models achieving >92%: {len(models_above_92)}\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 SENTIMENT ANALYSIS PROJECT COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}